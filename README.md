ğŸ“Œ Project Overview

This project implements a production-style Retail Data Warehouse ETL pipeline using Oracle Database, Python, and Apache Airflow.

The goal is to:

Extract transactional retail data from an Oracle source system

Validate data quality at every stage

Transform raw business attributes into surrogate-keyâ€“based dimensions

Load clean, analytics-ready data into a Target Data Warehouse (DW)

This project follows real-world Data Engineering best practices, including incremental loads, data validation, orchestration, and schema separation.

ğŸ—ï¸ Architecture Overview
Oracle OLTP (Source)
        |
        v
Daily Extraction (CSV Snapshots)
        |
        v
Data Validation (Files + Tables)
        |
        v
Transformation (Set-based Logic)
        |
        v
Oracle Target Data Warehouse (Star Schema)
        |
        v
Analytics / Reporting

ğŸ§± Data Layers Explained
1ï¸âƒ£ Source Layer (Oracle â€“ OLTP)

Tables like:

fact_sales

dim_store_master

dim_product

dim_distributor

dim_date

Data is read-only

No transformations happen here

2ï¸âƒ£ Extract Layer (Files)

Daily snapshot files are generated using SQL joins and stored as pipe-delimited CSVs:

/opt/airflow/data_extracts/
â”œâ”€â”€ incoming/
â”‚   â””â”€â”€ sales_snapshot_YYYYMMDD_HHMM.csv
â”œâ”€â”€ current/
â”œâ”€â”€ archive/


Why files?

Decouples source from warehouse

Allows reprocessing

Mimics real enterprise pipelines

3ï¸âƒ£ Validation Layer (Data Quality)

Validation is applied at two levels:

ğŸ”¹ File Validation

Mandatory column checks

Numeric column validation

Flag column validation (Y/N)

Minimum row count

Pipe (|) delimiter handling

ğŸ”¹ Table Validation

Row count thresholds

NULL checks on critical columns

Duplicate primary key checks

Freshness check for fact tables

If validation fails â†’ pipeline stops immediately.

4ï¸âƒ£ Target Data Warehouse (Oracle â€“ DW)

A Star Schema is implemented under a separate schema (target_dw).

Dimension Tables

dim_store_dw

dim_store_chain_dw

dim_product_dw

dim_category

dim_sub_category

dim_manufacturer

dim_distributor_dw

dim_date_dw

Fact Table

fact_sales_dw

Key Concepts Used

Surrogate keys

Business keys

Foreign key constraints

Incremental dimension loads

ğŸ”„ Incremental Dimension Load Logic (Set-Based)

Each dimension follows this pattern:

Read latest incoming file

Extract unique business keys

Fetch existing keys from DW into a dictionary (cache)

Identify new records only

Insert only new records

Update cache

Map surrogate keys back to main dataset

This approach is:

Fast

Scalable

Industry standard

ğŸ§ª Manufacturer & Category Handling

Manufacturers are derived logically based on product category using controlled mappings.

Example:

Grocery â†’ NestlÃ©, Tata Consumer, Britannia

BabyCare â†’ Johnson & Johnson, P&G

PersonalCare â†’ HUL, Dabur

This mimics real master data enrichment.

â±ï¸ Orchestration (Apache Airflow)
Key DAGs
ğŸ”¹ Extraction DAG

Generates daily snapshot files

ğŸ”¹ Validation DAG

Validates extracted files

Validates source tables

ğŸ”¹ Target DW Load DAG
load_dim_store_dw
    >> load_dim_product_dw
    >> load_dim_distributor_dw
    >> load_dim_date_dw
    >> load_fact_sales_dw


Retry logic enabled

Fail-fast on data issues

Fully automated (no manual runs required)

ğŸ“¦ Technologies Used
Category         |	Tools
Database      	 | Oracle Database
Orchestration	   | Apache Airflow
Language	       | Python
Libraries	       | pandas, oracledb
Containerization |	Docker
Scheduling	Cron via Airflow
â–¶ï¸ How to Run the Project
1ï¸âƒ£ Start Airflow
docker-compose up -d

2ï¸âƒ£ Verify Containers
docker ps

3ï¸âƒ£ Open Airflow UI
http://localhost:8080

4ï¸âƒ£ Trigger DAGs (in order)

Extract pipeline

Validation pipeline

Target DW load pipeline

ğŸ›¡ï¸ Error Handling

Common issues handled:

Invalid numeric values (DPY-4004)

Empty or malformed files

Missing foreign keys

Duplicate business keys

Partial data loads

Pipelines fail safely with clear logs.

ğŸ“Š Outcome

After completion:
Clean star-schema data in Oracle DW

Fully validated, analytics-ready tables

Re-runnable, auditable pipeline
